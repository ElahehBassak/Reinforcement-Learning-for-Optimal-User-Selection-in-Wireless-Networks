# Submitted Model 2: HDBSCAN for User-Grouping

## Model Overview

In the baseline code execution, we identified that a significant portion of the modelâ€™s runtime was spent on the 'user grouping' algorithm. To optimize this, we explored spatial clustering techniques to group uncorrelated users. Our submitted model utilizes **HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)** for user grouping. We also performed the following modifications to improve the SAC model's convergence and performance:

- State space: inadditoin to using HDBSCAN for user_grouping, we used the standard L2 norm of the user's channel vector instead of the ue_se_max. We standardized the L2 norm by subtracting its mean and dividing by its standard deviation. We also standardized the value of ue_history by similary subtracting its mean and dividing by its standard deviation. More Information can be found under "SAC Model" section
- Action Space: used num_actions=16 & max_actions=16. No KNN (even when tested with the KNN portion of the code, using HDBSCAN with normalized H & normalized ue_history overperformed the baseline). 
- Training: used both high mobility and low mobility datasets. Also, different subcarriers. Similar approach to model 1. More details provided in "Training Procedure" Section.

## 1. User Grouping with HDBSCAN
To enhance the user grouping algorithm implemented in the baseline, we used HDBSCAN to perform the clustering. HDBSCAN is a spatial clustering algorithm that automatically identifies clusters of varying densities without requiring the number of clusters to be specified. It handles noise effectively by labeling outliers and is well-suited for complex, high-dimensional datasets where cluster structure is unclear or complex. The function used to perform HDBSCAN is further explained below.

**Function:** `cluster_with_hdbscan(H, min_cluster_size=2)`

- **File:** `user_group_hdbscan.py`
- **Description:** This function clusters users based on their correlation matrix, grouping users with low correlation together.

**Parameters:**
- `H` (numpy array): The input channel matrix of size `64 x 64`.
- `min_cluster_size` (int, default=2): Minimum size of clusters considered by HDBSCAN. Smaller values allow for finer clustering, while larger values group users into larger clusters. We used `min_cluster_size=2`.
-Returns an array of cluster labels of size `64 x 1`

**Steps:**
1. **Correlation Matrix Calculation:** The helper function `corr(H)` is used to compute the channel correlation matrix. 

2. **Distance Matrix definition:** The correlation matrix from the previous step is used as the distance matrix. The diagonal of the distance matrix is set to 0 because the distance between an item and itself is not meaningful for clustering purposes.  For HDBSCAN, this matrix is provided as input to determine how to cluster the items.

3. **Applying HDBSCAN:** The HDBSCAN algorithm is initialized with the prepared distance matrix and the specified minimum cluster size. The model is then fit to the distance matrix, which performs the clustering based on the input data.

4. **Cluster Labels:** Cluster labels for each user are generated and returned. 

## 2. SAC Model

**State Space:**
- Utilizes standardized L2 norm of columns of `H` instead of `se_max_ur`. standardized L2 norm of H= (`H_norm`-`H_norm.mean()`)/(`H_norm.std()`+`1e-8`). 1e-8 is added to avoid dividing by zero. 
- Uses standardized `ue_history`, which is equal to (`ue_history`-`ue_history.mean()`)/(`ue_history.std()`+`1e-8`). 1e-8 is added to avoid dividing by zero. 
- Incorporates `group_idx` generated by HDBSCAN.
- `Num_states = 3 x 64 = 192`.

**Action Space:**
- Rationale provided in submitted model 1 (please refer to "Action" subsection under "SAC Agent" in model 1 README.md file):
  - `num_actions = 16`
  - `max_actions = 16`
  - No KNN used.
However, the baseline SAC model with KNN, num_actions = 8 & max_actions = 256, was tested with the modified input state shown above including HDBSCAN, and the achieved model sill outperformed the baseline model.

**Reward:**
- No changes from the baseline reward:
  - `a = b = 1`
  - `reward_scale = 0.1`.

**Actor-Critic Architecture**
- Baseline model - no change.

## 3. Training Procedure
- Follows the same approach described in "Submitted Mode 1". Please refer to "Training Procedure" section in "Submitted Mode 1" README.md file.
  - `maximum number of episodes = 1000`
  - `max_episode_steps = 500`
  - Apple M3 Pro MAC GPU: "mps"

## 4. Code
- Since the model has been trained on mac mps gpu, when loading the checkpoint to cuda based gpu please make sure to use the following command:
- `torch.load(ckpt_path, map_location=torch.device('cuda'))`

## References

1. [HDBSCAN API Documentation](https://hdbscan.readthedocs.io/en/latest/api.html)
2. [How HDBSCAN Works](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)
